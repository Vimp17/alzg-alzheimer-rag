from typing import List, Dict, Any, Optional
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings("ignore")

class RAGPipelineHF:
    def __init__(self, embedding_manager, response_generator):
        self.embedding_manager = embedding_manager
        self.response_generator = response_generator
        
    def query(self, question: str, n_sources: int = 5, 
              threshold: float = 0.5, include_summaries: bool = True) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ RAG pipeline —Å –æ–ø—Ü–∏–µ–π –≤–∫–ª—é—á–µ–Ω–∏—è –∫—Ä–∞—Ç–∫–∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–π
        """
        
        print(f"Step 1: Retrieving relevant documents for: '{question}'")
        retrieval_results = self.embedding_manager.search_similar(
            question, 
            n_results=n_sources * 3  # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–æ–ª—å—à–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–±–æ—Ä–∞
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not retrieval_results or not retrieval_results.get('documents'):
            print("No documents found in retrieval.")
            return {
                "question": question,
                "answer": "No relevant documents found in the database. Please make sure the database is populated with articles.",
                "sources": [],
                "confidence": 0.0,
                "context_used": [],
                "metadata": {
                    "n_sources_retrieved": 0,
                    "n_sources_used": 0,
                    "retrieval_method": "semantic_search",
                    "error": "No documents in database"
                }
            }
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –µ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
        if not retrieval_results['documents'][0]:
            print("Empty documents list.")
            return {
                "question": question,
                "answer": "The search returned no results. The database might be empty.",
                "sources": [],
                "confidence": 0.0,
                "context_used": [],
                "metadata": {
                    "n_sources_retrieved": 0,
                    "n_sources_used": 0,
                    "retrieval_method": "semantic_search",
                    "error": "Empty documents list"
                }
            }
        
        print(f"Found {len(retrieval_results['documents'][0])} initial documents.")
        
        # 2. Re-ranking - —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        print("Step 2: Re-ranking results...")
        try:
            reranked_results = self.rerank_with_cross_encoder(
                question, 
                retrieval_results
            )
        except Exception as e:
            print(f"Error in re-ranking: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –µ—Å–ª–∏ —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
            reranked_results = retrieval_results
        
        # 3. Filtering - —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–æ—Ä–æ–≥—É
        print("Step 3: Filtering results...")
        filtered_results = self.filter_by_threshold(
            reranked_results, 
            threshold
        )
        
        if not filtered_results['documents'][0]:
            return {
                "question": question,
                "answer": "No sufficiently relevant sources found. Please try rephrasing your question or lowering the similarity threshold.",
                "sources": [],
                "confidence": 0.0,
                "context_used": [],
                "metadata": {
                    "n_sources_retrieved": len(retrieval_results['documents'][0]),
                    "n_sources_used": 0,
                    "retrieval_method": "semantic_search",
                    "error": "Below threshold"
                }
            }
        
        print(f"After filtering: {len(filtered_results['documents'][0])} relevant documents.")
        
        # 4. Context aggregation - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        print("Step 4: Preparing context with summaries...")
        final_contexts, final_metadatas = self.aggregate_context_with_summaries(
            filtered_results, 
            max_chunks=n_sources,
            include_summaries=include_summaries
        )
        
        # 5. Generation - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        print("Step 5: Generating answer with source summaries...")
        generation_result = self.response_generator.generate_answer(
            question,
            final_contexts,
            final_metadatas
        )
        
        # 6. Source attribution - –∞—Ç—Ä–∏–±—É—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –∫—Ä–∞—Ç–∫–∏–º–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è–º–∏
        print("Step 6: Attributing sources with summaries...")
        attributed_sources = self.attribute_sources_with_summaries(
            generation_result['answer'],
            final_contexts,
            final_metadatas
        )
        
        # 7. Confidence calculation - —Ä–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidence = 0.7  # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if filtered_results['distances'][0]:
            avg_similarity = np.mean(filtered_results['distances'][0])
            confidence = min(avg_similarity * 1.2, 0.95)  # –ú–∞–∫—Å–∏–º—É–º 95%
        
        # 8. –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –∫—Ä–∞—Ç–∫–∏–º–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è–º–∏
        final_answer = self.enhance_answer_with_summaries(
            generation_result['answer'],
            attributed_sources,
            include_summaries
        )
        
        return {
            "question": question,
            "answer": final_answer,
            "sources": attributed_sources,
            "confidence": confidence,
            "context_used": final_contexts[:3],
            "metadata": {
                "n_sources_retrieved": len(retrieval_results['documents'][0]),
                "n_sources_used": len(final_contexts),
                "retrieval_method": "semantic_search + cross-encoder",
                "generation_model": self.response_generator.__class__.__name__,
                "include_summaries": include_summaries
            }
        }
    
    def rerank_with_cross_encoder(self, query, results):
        """–†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–∞"""
        try:
            from sentence_transformers import CrossEncoder
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if not results or not results.get('documents') or not results['documents'][0]:
                return results
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç
            if len(results['documents'][0]) == 0:
                return results
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            
            pairs = []
            for doc in results['documents'][0]:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ None –∏ –Ω–µ –ø—É—Å—Ç–æ–π
                if doc and len(doc.strip()) > 0:
                    pairs.append([query, doc])
            
            # –ï—Å–ª–∏ –Ω–µ—Ç –ø–∞—Ä –¥–ª—è –æ—Ü–µ–Ω–∫–∏
            if not pairs:
                return results
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–∞
            scores = cross_encoder.predict(pairs)
            
            # –ï—Å–ª–∏ scores –ø—É—Å—Ç
            if len(scores) == 0:
                return results
            
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—á–µ—Ç–∞
            sorted_indices = np.argsort(scores)[::-1]
            
            # –†–µ—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            reranked = {
                'documents': [[results['documents'][0][i] for i in sorted_indices if i < len(results['documents'][0])]],
                'metadatas': [[results['metadatas'][0][i] for i in sorted_indices if i < len(results['metadatas'][0])]],
                'distances': [[results['distances'][0][i] for i in sorted_indices if i < len(results['distances'][0])]],
            }
            
            return reranked
            
        except Exception as e:
            print(f"Cross-encoder reranking failed: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            return results
    
    def filter_by_threshold(self, results, threshold=0.5):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –ø–æ—Ä–æ–≥—É —Å—Ö–æ–∂–µ—Å—Ç–∏"""
        filtered_docs = []
        filtered_metas = []
        filtered_dists = []
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
        if not results or not results.get('documents') or not results['documents'][0]:
            return {
                'documents': [[]],
                'metadatas': [[]],
                'distances': [[]]
            }
        
        for doc, meta, dist in zip(results['documents'][0], 
                                  results['metadatas'][0], 
                                  results['distances'][0]):
            if dist >= threshold:
                filtered_docs.append(doc)
                filtered_metas.append(meta)
                filtered_dists.append(dist)
        
        return {
            'documents': [filtered_docs],
            'metadatas': [filtered_metas],
            'distances': [filtered_dists]
        }
    
    def aggregate_context_with_summaries(self, results, max_chunks=5, include_summaries=True):
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫—Ä–∞—Ç–∫–∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–π"""
        unique_contexts = []
        unique_metadatas = []
        
        if not results or not results['documents'][0]:
            return [], []
        
        for doc, meta in zip(results['documents'][0], results['metadatas'][0]):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–∏–∑–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            is_duplicate = False
            for existing_doc in unique_contexts:
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –ø–µ—Ä–≤—ã–º 100 —Å–∏–º–≤–æ–ª–∞–º
                if doc[:100] in existing_doc or existing_doc[:100] in doc:
                    is_duplicate = True
                    break
            
            if not is_duplicate and len(unique_contexts) < max_chunks:
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                if 'abstract' in meta:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º abstract –∫–∞–∫ summary
                    meta['summary'] = meta['abstract'][:300] + "..." if len(meta['abstract']) > 300 else meta['abstract']
                elif 'summary' not in meta:
                    # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–∑ –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
                    sentences = doc.split('. ')
                    summary = '. '.join(sentences[:2]) + '.' if len(sentences) > 1 else doc[:200] + "..."
                    meta['summary'] = summary
                
                unique_contexts.append(doc)
                unique_metadatas.append(meta)
        
        return unique_contexts, unique_metadatas
    
    def attribute_sources_with_summaries(self, answer, contexts, metadatas):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö —Å –∫—Ä–∞—Ç–∫–∏–º–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è–º–∏"""
        sources = []
        
        if not contexts or not metadatas:
            return sources
        
        for i, (context, metadata) in enumerate(zip(contexts, metadatas)):
            # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º summary
            summary = metadata.get('summary', '')
            if not summary:
                sentences = context.split('. ')
                summary = '. '.join(sentences[:2]) + '.' if len(sentences) > 1 else context[:200] + "..."
            
            source_info = {
                "source_id": i + 1,
                "title": metadata.get('title', 'Unknown'),
                "authors": metadata.get('authors', 'Unknown authors'),
                "journal": metadata.get('journal', 'Unknown journal'),
                "year": metadata.get('year', 'Unknown year'),
                "doi": metadata.get('doi', ''),
                "url": metadata.get('url', ''),
                "summary": summary,  # –î–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
                "excerpt": context[:300] + "..." if len(context) > 300 else context,
                "relevance_score": metadata.get('relevance_score', 0.9 - (i * 0.1)),
                "similarity": metadata.get('distance', 0.8 - (i * 0.1))
            }
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —Ü–∏—Ç–∏—Ä—É–µ—Ç—Å—è –ª–∏ —ç—Ç–æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ –æ—Ç–≤–µ—Ç–µ
            if f"[Source {i+1}]" in answer or f"[{i+1}]" in answer:
                source_info["cited"] = True
                source_info["citation_count"] = answer.count(f"[Source {i+1}]")
            else:
                source_info["cited"] = False
                source_info["citation_count"] = 0
            
            sources.append(source_info)
        
        return sources
    
    def enhance_answer_with_summaries(self, answer, sources, include_summaries=True):
        """–£–ª—É—á—à–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ–º –∫—Ä–∞—Ç–∫–∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        if not include_summaries or not sources:
            return answer
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–¥–µ–ª —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        enhanced_answer = f"{answer}\n\n{'='*60}\nüìö **REFERENCED ARTICLES**\n{'='*60}\n"
        
        for source in sources:
            if source.get('cited', False):
                enhanced_answer += f"\n**üìñ [Source {source['source_id']}] {source['title']}**\n"
                enhanced_answer += f"üë• *Authors:* {source['authors']}\n"
                enhanced_answer += f"üìÖ *Year:* {source['year']}\n"
                enhanced_answer += f"üìù *Summary:* {source['summary']}\n"
                enhanced_answer += f"üìä *Relevance:* {source['relevance_score']:.2f}/1.0\n"
                enhanced_answer += f"üîó *Citations in answer:* {source['citation_count']}\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        uncited_sources = [s for s in sources if not s.get('cited', False)]
        if uncited_sources:
            enhanced_answer += f"\n{'='*60}\nüìñ **ADDITIONAL RELEVANT ARTICLES**\n{'='*60}\n"
            for source in uncited_sources[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 3 —Å—Ç–∞—Ç—å—è–º–∏
                enhanced_answer += f"\n‚Ä¢ **{source['title']}** ({source['year']}) - {source['summary'][:150]}...\n"
        
        enhanced_answer += f"\n{'='*60}\n"
        enhanced_answer += "üìå *Note: This response is based on analysis of research articles. Consult original sources for complete information.*"
        
        return enhanced_answer
    
    def get_article_summary(self, article_id: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏"""
        # –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç—å–∏ –ø–æ ID
        results = self.embedding_manager.search_similar(
            article_id, 
            n_results=1
        )
        
        if not results or not results['documents'][0]:
            return {"error": "Article not found"}
        
        metadata = results['metadatas'][0][0]
        document = results['documents'][0][0]
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ summary
        title = metadata.get('title', 'Unknown')
        authors = metadata.get('authors', 'Unknown')
        year = metadata.get('year', 'Unknown')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = document.split('. ')
        key_sentences = []
        
        # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
        keywords = ['find', 'show', 'demonstrate', 'conclude', 'suggest', 'indicate', 'reveal']
        for sentence in sentences:
            if any(keyword in sentence.lower() for keyword in keywords):
                key_sentences.append(sentence.strip())
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–ª–∏ –ø–µ—Ä–≤—ã–µ 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if len(key_sentences) >= 3:
            summary = '. '.join(key_sentences[:3]) + '.'
        else:
            summary = '. '.join(sentences[:3]) + '.'
        
        return {
            "article_id": article_id,
            "title": title,
            "authors": authors,
            "year": year,
            "summary": summary,
            "full_context": document[:500] + "..." if len(document) > 500 else document,
            "metadata": metadata
        }