import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    pipeline,
    StoppingCriteria,
    StoppingCriteriaList
)
from typing import List, Dict, Any
import re

class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_token_ids):
        self.stop_token_ids = stop_token_ids
    
    def __call__(self, input_ids, scores, **kwargs):
        for stop_id in self.stop_token_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False

class HFResponseGenerator:
    def __init__(self, model_name="microsoft/phi-2", device_map="auto"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"Loading generation model on {self.device}...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏
        if "t5" in model_name.lower():
            self.model_type = "seq2seq"
            self.model_class = AutoModelForSeq2SeqLM
        else:
            self.model_type = "causal"
            self.model_class = AutoModelForCausalLM
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left"
        )
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ pad token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
        if self.tokenizer.pad_token is None:
            if self.tokenizer.eos_token is not None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            else:
                self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
        load_kwargs = {
            "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32,
            "trust_remote_code": True,
        }
        
        # –¢–æ–ª—å–∫–æ –¥–ª—è GPU –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map, –¥–ª—è CPU - –Ω–µ—Ç
        if self.device == "cuda":
            load_kwargs["device_map"] = device_map
        else:
            # –î–ª—è CPU –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º device_map
            load_kwargs["device_map"] = None
        
        # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32
        if self.device == "cpu":
            load_kwargs["torch_dtype"] = torch.float32
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        try:
            self.model = self.model_class.from_pretrained(
                model_name,
                **load_kwargs
            )
        except Exception as e:
            print(f"Error loading {model_name}: {e}")
            # Fallback to distilgpt2
            print("Falling back to distilgpt2...")
            model_name = "distilgpt2"
            self.model_type = "causal"
            self.model_class = AutoModelForCausalLM
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.tokenizer.pad_token = self.tokenizer.eos_token
            load_kwargs["torch_dtype"] = torch.float32
            load_kwargs["device_map"] = None
            self.model = self.model_class.from_pretrained(
                model_name,
                **load_kwargs
            )
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU, –ø–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Ä—É—á–Ω—É—é
        if self.device == "cuda" and not next(self.model.parameters()).is_cuda:
            self.model = self.model.cuda()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ pipeline –ë–ï–ó —É–∫–∞–∑–∞–Ω–∏—è device
        if self.model_type == "seq2seq":
            self.generator = pipeline(
                "text2text-generation",  # –î–ª—è T5 –∏—Å–ø–æ–ª—å–∑—É–µ–º text2text-generation
                model=self.model,
                tokenizer=self.tokenizer,
                # –ù–µ —É–∫–∞–∑—ã–≤–∞–µ–º device –∑–¥–µ—Å—å!
            )
        else:
            self.generator = pipeline(
                "text-generation",  # –î–ª—è GPT-like –º–æ–¥–µ–ª–µ–π
                model=self.model,
                tokenizer=self.tokenizer,
                # –ù–µ —É–∫–∞–∑—ã–≤–∞–µ–º device –∑–¥–µ—Å—å!
            )
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.generation_config = {
            "max_new_tokens": 800,  # –£–≤–µ–ª–∏—á–∏–º –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            "temperature": 0.3,     # –°–Ω–∏–∑–∏–º –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            "top_p": 0.9,
            "top_k": 40,
            "do_sample": True,
            "repetition_penalty": 1.2,
            "no_repeat_ngram_size": 3,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
        }
    
    def format_prompt(self, query: str, contexts: List[str], 
                     metadatas: List[Dict]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –≤–∫–ª—é—á–µ–Ω–∏–µ–º –∫—Ä–∞—Ç–∫–∏—Ö —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–π"""
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∫—Ä–∞—Ç–∫–∏–º —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ–º
        context_str = ""
        for i, (context, metadata) in enumerate(zip(contexts, metadatas), 1):
            # –ü–æ–ª—É—á–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            summary = metadata.get('summary', '')
            if not summary and 'abstract' in metadata:
                summary = metadata['abstract']
            
            # –ï—Å–ª–∏ –Ω–µ—Ç summary/abstract, —Å–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if not summary:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∫ summary
                sentences = context.split('. ')
                summary = '. '.join(sentences[:2]) + '.' if len(sentences) > 1 else context[:200] + "..."
            
            source_info = f"[Source {i}]"
            if 'title' in metadata:
                source_info += f" Title: {metadata['title']}"
            if 'year' in metadata:
                source_info += f" ({metadata['year']})"
            
            context_str += f"\n\n{'='*80}\n{source_info}\n{'-'*80}"
            context_str += f"\nüìù Article Summary: {summary[:300]}..."
            context_str += f"\nüìÑ Relevant Excerpt: {context[:500]}..."
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = """You are an expert biomedical research assistant specializing in Alzheimer's disease. 
Your task is to provide a comprehensive, evidence-based answer using ONLY the provided research excerpts.

IMPORTANT INSTRUCTIONS:
1. Start with a clear, concise summary answer
2. For each key point, cite the specific source using [Source X] notation
3. If multiple sources support a claim, cite all relevant sources [Source X, Source Y]
4. If information is missing from provided sources, explicitly state this
5. Structure your answer logically with clear paragraphs
6. Include a brief conclusion summarizing the key findings

CRITICAL: Do not make up any information not present in the provided sources."""
        
        # –†–∞–∑–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π
        if self.model_type == "seq2seq":
            # –î–ª—è T5
            prompt = f"""Based on the following research article summaries and excerpts, answer the query:

Query: {query}

Research Sources:{context_str}

Provide a detailed, evidence-based answer that cites specific sources:"""
        else:
            # –î–ª—è GPT-like –º–æ–¥–µ–ª–µ–π
            prompt = f"""{system_prompt}

RESEARCH QUESTION: {query}

RESEARCH SOURCES (with summaries and relevant excerpts):{context_str}

YOUR TASK: Based ONLY on the above research sources, provide a comprehensive answer that:
1. Answers the research question
2. Cites specific sources for each claim
3. Summarizes key findings from each relevant source

ANSWER STRUCTURE:
- Brief overall summary
- Detailed analysis with source citations
- Conclusion with key takeaways

BEGIN ANSWER:"""
        
        return prompt
    
    def generate_answer(self, query: str, contexts: List[str], 
                       metadatas: List[Dict]) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        if not contexts:
            return {
                "answer": "No relevant research context provided to answer this question.",
                "prompt_used": "",
                "model_type": self.model_type,
                "success": False
            }
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
        prompt = self.format_prompt(query, contexts, metadatas)
        
        print(f"Generated prompt length: {len(prompt)} characters")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
            outputs = self.generator(
                prompt,
                **self.generation_config
            )
            
            generated_text = outputs[0]['generated_text']
            
            # –î–ª—è causal –º–æ–¥–µ–ª–µ–π —É–¥–∞–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑ –Ω–∞—á–∞–ª–∞ –æ—Ç–≤–µ—Ç–∞
            if self.model_type == "causal" and generated_text.startswith(prompt):
                answer = generated_text[len(prompt):].strip()
            else:
                answer = generated_text.strip()
            
            print(f"Generated answer length: {len(answer)} characters")
            
            # –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            answer = self.postprocess_answer(answer, contexts, metadatas)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
            if len(answer) < 100:
                print("Answer too short, enriching with source information...")
                answer = self.enrich_short_answer(answer, contexts, metadatas)
            
            return {
                "answer": answer,
                "prompt_used": prompt[:500] + "..." if len(prompt) > 500 else prompt,
                "model_type": self.model_type,
                "success": True
            }
            
        except Exception as e:
            print(f"Error in generation: {e}")
            import traceback
            traceback.print_exc()
            
            # Fallback –æ—Ç–≤–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
            fallback_answer = self.create_fallback_answer(query, contexts, metadatas)
            
            return {
                "answer": fallback_answer,
                "prompt_used": prompt[:500] + "...",
                "model_type": self.model_type,
                "success": False,
                "error": str(e)
            }
    
    def postprocess_answer(self, answer: str, contexts: List[str], 
                          metadatas: List[Dict]) -> str:
        """–ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = answer.split('. ')
        unique_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in unique_sentences:
                unique_sentences.append(sentence)
        
        answer = '. '.join(unique_sentences)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        answer = answer.strip()
        
        return answer
    
    def enrich_short_answer(self, answer: str, contexts: List[str], 
                           metadatas: List[Dict]) -> str:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        enriched = f"{answer}\n\n"
        enriched += "üîç **Additional Information from Sources:**\n\n"
        
        for i, (context, metadata) in enumerate(zip(contexts, metadatas), 1):
            title = metadata.get('title', f"Source {i}")
            authors = metadata.get('authors', 'Unknown authors')
            year = metadata.get('year', 'Unknown year')
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
            summary = metadata.get('summary', '')
            if not summary:
                sentences = context.split('. ')
                summary = '. '.join(sentences[:2]) + '.' if len(sentences) > 1 else context[:150] + "..."
            
            enriched += f"**[{i}] {title}** ({year}, {authors})\n"
            enriched += f"üìù *Summary:* {summary}\n\n"
        
        return enriched
    
    def create_fallback_answer(self, query: str, contexts: List[str], 
                              metadatas: List[Dict]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ fallback –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        answer = f"Based on the {len(contexts)} relevant research articles, here's what I found for your query about '{query}':\n\n"
        
        for i, (context, metadata) in enumerate(zip(contexts, metadatas), 1):
            title = metadata.get('title', f"Article {i}")
            year = metadata.get('year', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            key_points = []
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ
            keywords = ['inhibitors', 'treatment', 'therapy', 'target', 'mechanism', 'effect']
            sentences = context.split('. ')
            for sentence in sentences:
                if any(keyword in sentence.lower() for keyword in keywords):
                    key_points.append(sentence.strip())
            
            if key_points:
                answer += f"**Source {i}: {title}** {f'({year})' if year else ''}\n"
                answer += f"‚Ä¢ {key_points[0]}\n"
                if len(key_points) > 1:
                    answer += f"‚Ä¢ {key_points[1]}\n"
                answer += "\n"
        
        answer += "\n‚ö†Ô∏è *Note: This is an automatically generated summary based on available research excerpts.*"
        
        return answer