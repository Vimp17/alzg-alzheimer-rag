import json
import chromadb
from sentence_transformers import SentenceTransformer
import os
from chromadb.config import Settings

def init_database():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º abstract"""
    print("Initializing database with demo data...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    os.makedirs("./data/embeddings", exist_ok=True)
    os.makedirs("./data/raw", exist_ok=True)
    
    # –î–µ–º–æ-—Å—Ç–∞—Ç—å–∏ –æ –±–æ–ª–µ–∑–Ω–∏ –ê–ª—å—Ü–≥–µ–π–º–µ—Ä–∞
    demo_articles = [
        {
            "pmid": "demo_001",
            "title": "Tau protein aggregation in Alzheimer's disease",
            "abstract": "This review discusses tau protein aggregation mechanisms and potential therapeutic targets including GSK3Œ≤ and CDK5 inhibitors. The study examines phosphorylation patterns and their role in neurofibrillary tangle formation.",
            "authors": ["Smith J", "Johnson R"],
            "year": "2023",
            "journal": "Nature Reviews Neurology",
            "doi": "10.1038/s41582-023-00789-7",
            "url": "https://example.com/tau-review"
        },
        {
            "pmid": "demo_002", 
            "title": "Amyloid-beta clearance pathways and therapeutic interventions",
            "abstract": "Research on amyloid-beta clearance through the glymphatic system and potential drug targets including BACE1 inhibitors and gamma-secretase modulators. The study highlights novel clearance mechanisms.",
            "authors": ["Brown K", "Davis M"],
            "year": "2022",
            "journal": "Science",
            "doi": "10.1126/science.abc1234",
            "url": "https://example.com/amyloid-review"
        },
        # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
    ]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ–º–æ-—Å—Ç–∞—Ç—å–∏
    with open('./data/raw/demo_articles.json', 'w') as f:
        json.dump(demo_articles, f, indent=2)
    
    print("Demo articles saved.")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ChromaDB
    client = chromadb.PersistentClient(
        path="./data/embeddings/chroma_db",
        settings=Settings(
            anonymized_telemetry=False,
            allow_reset=True
        )
    )
    
    # –û—á–∏—â–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    try:
        client.delete_collection("alzheimer_research")
        print("Old collection deleted.")
    except:
        print("No existing collection found.")
    
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é
    collection = client.create_collection(
        name="alzheimer_research",
        metadata={
            "hnsw:space": "cosine",
            "description": "Alzheimer's disease research articles"
        }
    )
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("Loading embedding model...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ —Å—Ç–∞—Ç–µ–π
    chunks = []
    metadatas = []
    
    for article in demo_articles:
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏ –∞–±—Å—Ç—Ä–∞–∫—Ç–∞
        text = f"Title: {article['title']}\n\nAbstract: {article['abstract']}"
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences = text.split('. ')
        chunk = ""
        
        for sentence in sentences:
            if len(chunk) + len(sentence) < 500:
                chunk += sentence + ". "
            else:
                if chunk.strip():
                    chunks.append(chunk.strip())
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –∞–≤—Ç–æ—Ä–æ–≤ –≤ —Å—Ç—Ä–æ–∫—É
                    authors_str = ", ".join(article['authors']) if isinstance(article['authors'], list) else str(article['authors'])
                    metadatas.append({
                        'title': article['title'],
                        'authors': authors_str,
                        'journal': article['journal'],
                        'year': article['year'],
                        'doi': article['doi'],
                        'url': article['url'],
                        'article_id': article['pmid'],
                        'type': 'research_article',
                        'abstract': article['abstract'],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π abstract
                        'summary': article['abstract'][:200] + "..." if len(article['abstract']) > 200 else article['abstract']
                    })
                chunk = sentence + ". "
        
        if chunk.strip():
            chunks.append(chunk.strip())
            authors_str = ", ".join(article['authors']) if isinstance(article['authors'], list) else str(article['authors'])
            metadatas.append({
                'title': article['title'],
                'authors': authors_str,
                'journal': article['journal'],
                'year': article['year'],
                'doi': article['doi'],
                'url': article['url'],
                'article_id': article['pmid'],
                'type': 'research_article',
                'abstract': article['abstract'],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π abstract
                'summary': article['abstract'][:200] + "..." if len(article['abstract']) > 200 else article['abstract']
            })
    
    print(f"Created {len(chunks)} chunks from {len(demo_articles)} articles.")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —á–∞–Ω–∫–∏ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    if len(chunks) == 0:
        print("Error: No chunks created. Exiting.")
        return False
    
    # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    print("Creating embeddings...")
    embeddings = model.encode(chunks, show_progress_bar=True, normalize_embeddings=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print(f"Embeddings shape: {embeddings.shape}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    print("Adding to ChromaDB...")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è
    embeddings_list = embeddings.tolist()
    ids = [f"chunk_{i}" for i in range(len(chunks))]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤
    if len(embeddings_list) != len(chunks) or len(chunks) != len(metadatas):
        print(f"Error: Mismatch in data sizes: embeddings={len(embeddings_list)}, chunks={len(chunks)}, metadatas={len(metadatas)}")
        return False
    
    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    collection.add(
        embeddings=embeddings_list,
        documents=chunks,
        metadatas=metadatas,
        ids=ids
    )
    
    print(f"‚úÖ Database initialized with {len(chunks)} chunks.")
    print(f"‚úÖ Collection 'alzheimer_research' created.")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    count = collection.count()
    print(f"‚úÖ Total documents in collection: {count}")
    
    return True

if __name__ == "__main__":
    success = init_database()
    if success:
        print("\nüéâ Database initialization completed successfully!")
        print("üìö Articles include full abstracts for better summarization.")
    else:
        print("\n‚ùå Database initialization failed!")